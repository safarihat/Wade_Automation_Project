# doc_generator/services/vulnerability_service.py
import json
import logging
import time
import functools
import hashlib
from typing import List, Dict, Any, Optional, Tuple

from django.conf import settings
from django.core.cache import cache

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_groq import ChatGroq

from doc_generator.services.embedding_service import get_embedding_model
from .retrieval_service import RetrievalService

logger = logging.getLogger(__name__)


class CustomJSONEncoder(json.JSONEncoder):
    """
    A custom JSON encoder that handles LangChain Document objects and other non-serializable types.
    """
    def default(self, o):
        if isinstance(o, Document):
            # Represent Document objects as a dictionary
            return {
                "page_content": o.page_content,
                "metadata": o.metadata,
            }
        try:
            return super().default(o)
        except TypeError:
            # For any other non-serializable object, return its string representation
            return str(o)


def log_execution_details(func):
    """
    A decorator to log function execution time and estimated output tokens.
    Uses a custom JSON encoder to handle non-serializable objects like LangChain Documents.
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        result, error = func(*args, **kwargs)
        end_time = time.perf_counter()
        latency = end_time - start_time

        output_token_estimate = 0
        if isinstance(result, str):
            output_token_estimate = len(result) // 4
        elif isinstance(result, dict):
            try:
                # Use the custom encoder to handle complex objects
                serialized_result = json.dumps(result, cls=CustomJSONEncoder)
                output_token_estimate = len(serialized_result) // 4
            except Exception as e:
                logger.warning(f"Could not serialize result for output token estimation in {func.__name__}: {e}")

        input_token_estimate = 0
        # Attempt to estimate input tokens from kwargs, assuming 'inputs' key for LLM chains
        if 'inputs' in kwargs and isinstance(kwargs['inputs'], dict):
            try:
                input_str = json.dumps(kwargs['inputs'], cls=CustomJSONEncoder)
                input_token_estimate = len(input_str) // 4
            except Exception:
                pass # Ignore if input cannot be serialized for token estimation

        # Log raw LLM output (truncated if too long)
        raw_llm_output = ""
        if result and hasattr(result, 'content'):
            raw_llm_output = result.content
        elif isinstance(result, str):
            raw_llm_output = result
        elif isinstance(result, dict): # For cases where result is already a dict (e.g., parsed JSON)
            try:
                raw_llm_output = json.dumps(result, cls=CustomJSONEncoder)
            except Exception:
                raw_llm_output = str(result) # Fallback to string representation

        truncated_output = (raw_llm_output[:500] + '...') if len(raw_llm_output) > 500 else raw_llm_output
        logger.info(f"LLM Raw Output (truncated for {func.__name__}): {truncated_output}")

        logger.info(
            f"Executed {func.__name__}: Latency={latency:.2f}s, Status={'OK' if not error else 'ERROR'}, "
            f"Estimated Input Tokens={input_token_estimate}, Estimated Output Tokens={output_token_estimate}"
        )
        return result, error
    return wrapper


class VulnerabilityService:
    """
    Resilient, step-isolated vulnerability analysis orchestration.
    Each step is a standalone function, and the main orchestrator
    collects results and errors into a structured report.
    """

    CONTEXT_TOKEN_LIMIT = getattr(settings, "RAG_CONTEXT_TOKEN_LIMIT", 2000)
    SUMMARY_WORD_TARGET = getattr(settings, "RAG_SUMMARY_WORD_TARGET", 300)
    CACHE_TTL = getattr(settings, "RAG_CACHE_TTL", 60 * 60 * 24)

    def __init__(self, retriever, site_context: Dict[str, Any], embedding_model=None):
        self.retriever = retriever
        self.raw_site_context = site_context
        self.site_context = self._preprocess_site_context(site_context)
        self.council = self.site_context.get("council_authority_name", "the relevant region")
        self.catchment = self.site_context.get("catchment_name", "the relevant catchment")
        self.plan_id = self.raw_site_context.get("pk") or self.raw_site_context.get("plan_pk") or "UnknownPlanID"
        self.embedding_model = embedding_model or get_embedding_model()

        self.fast_llm = ChatGroq(
            model_name=getattr(settings, "GROQ_FAST_MODEL", "llama-3.1-8b-instant"),
            groq_api_key=settings.GROQ_API_KEY
        )
        powerful_model_name = getattr(settings, "GROQ_MODEL", "llama-3.1-8b-instant")
        self.powerful_llm = ChatGroq(
            model_name=powerful_model_name,
            groq_api_key=settings.GROQ_API_KEY
        )

        logger.info(f"PlanID={self.plan_id} - VulnerabilityService fast model: {getattr(self.fast_llm, 'model_name', 'unknown')}")
        logger.info(f"PlanID={self.plan_id} - VulnerabilityService powerful model: {getattr(self.powerful_llm, 'model_name', powerful_model_name)}")

        # -------------------------
        # Helpers
        # -------------------------
    def _preprocess_site_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
            if not isinstance(context, dict):
                return {}
            return {k: v for k, v in context.items() if v is not None and v != ''}

    def _safe_invoke(self, chain, inputs: Dict[str, Any], attempts: int = 2, pause_seconds: float = 1.0) -> Tuple[Any, Optional[str]]:
            last_exc = None
            for attempt in range(1, attempts + 1):
                try:
                    logger.debug(f"PlanID={self.plan_id} - Safe invoke attempt {attempt} for chain starting with {chain.first.__class__.__name__}")
                    result = chain.invoke(inputs)
                    return result, None
                except Exception as e:
                    last_exc = e
                    logger.warning(f"PlanID={self.plan_id} - Invocation attempt {attempt} failed: {e}")
                    time.sleep(pause_seconds)
                    continue
            
            error_message = f"LLM invocation failed after {attempts} attempts. Last error: {last_exc}"
            logger.error(f"PlanID={self.plan_id} - {error_message}")
            return None, error_message

    def _validate_output(self, text: str, context_str: str, step_name: str) -> Tuple[str, str, Optional[float]]:
        """
        Performs a lightweight check on LLM output quality, with a semantic fallback.
        Returns a status ('Valid', 'Degraded', 'Invalid'), a reason, and an optional similarity score.
        """
        # 1. Invalid check (empty, conversational, refusal)
        text_lower = text.lower()
        invalid_phrases = ["as an ai", "i cannot", "i am a large language model", "not possible to provide", "unknown - final analysis failed"]
        if not text or any(phrase in text_lower for phrase in invalid_phrases) or len(text) < 50:
            reason = f"Output for {step_name} is empty, conversational, or a refusal."
            logger.warning(f"PlanID={self.plan_id}, Step={step_name} - Validation failed: {reason}")
            return "Invalid", reason, None

        # 2. Degraded check (lack of context)
        if not context_str:
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Output validation skipped (no context to compare against)." )
            return "Valid", "Validation skipped: no context to compare against.", None

        context_lower = context_str.lower()
        
        # Check for specific regional terms
        regional_terms = [self.council.lower(), self.catchment.lower(), "southland"]
        regional_term_present = any(term in text_lower for term in regional_terms if term and term != "the relevant region")

        # Check for a sample of long words from the context
        context_words = set(context_lower.split())
        long_words = {word.strip(".,!?:;()[]") for word in context_words if len(word) > 9}
        keyword_match_ok = True
        if len(long_words) > 10:
            sample_words = list(long_words)[:10]
            matches = sum(1 for word in sample_words if word in text_lower)
            if matches < 2:
                keyword_match_ok = False

        # If keyword checks pass, output is valid
        if regional_term_present and keyword_match_ok:
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Output context verified by keywords.")
            return "Valid", "Output appears to be based on retrieved context.", None

        # If keyword checks fail, fall back to semantic similarity
        logger.info(f"PlanID={self.plan_id}, Step={step_name} - Keyword validation failed. Falling back to semantic similarity check.")
        similarity_score = self._get_embedding_similarity(text, context_str, step_name)
        
        if similarity_score > 0.75:
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Semantic check passed (score: {similarity_score}). Upgrading status to Valid.")
            return "Valid", f"Keyword check failed but semantic similarity was high ({similarity_score:.2f}).", similarity_score
        else:
            reason = f"Output failed keyword checks and semantic similarity was low ({similarity_score:.2f})."
            logger.warning(f"PlanID={self.plan_id}, Step={step_name} - Validation failed: {reason}")
            return "Degraded", reason, similarity_score

    def _get_embedding_similarity(self, text_a: str, text_b: str, step_name: str) -> float:
        """
        Computes cosine similarity between the embeddings of two texts.
        Returns a float between 0.0 and 1.0.
        """
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np

        text_a_chunk = text_a[:4000]
        text_b_chunk = text_b[:4000]

        cache_key = f"embedding_sim:{hashlib.sha256((text_a_chunk + text_b_chunk).encode()).hexdigest()}"
        cached_score = cache.get(cache_key)
        if cached_score is not None:
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Using cached embedding similarity score: {cached_score:.2f}")
            return cached_score

        try:
            embeddings = self.embedding_model.embed_documents([text_a_chunk, text_b_chunk])
            score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            cache.set(cache_key, score, timeout=self.CACHE_TTL)
            return float(score)
        except Exception as e:
            logger.error(f"PlanID={self.plan_id}, Step={step_name} - Failed to compute embedding similarity: {e}", exc_info=True)
            return 0.0

        # ----------------------------------
        # Step 1: Advanced Retrieval
        # ----------------------------------
    def _log_retrieval_metrics(self, queries: List[str], docs: List[Document], combined_context: str):
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        import datetime

        if not docs or not queries:
            return

        # 1. Calculate average cosine similarity
        try:
            query_embeddings = self.embedding_model.embed_documents(queries)
            doc_embeddings = self.embedding_model.embed_documents([d.page_content for d in docs])
            
            # Calculate similarity of each query to each doc
            similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)
            
            # For each query, find the top 5 doc similarities
            top_5_similarities = []
            for query_sims in similarity_matrix:
                # Get top 5 scores for this query
                top_scores = sorted(query_sims, reverse=True)[:5]
                if top_scores:
                    top_5_similarities.append(np.mean(top_scores))

            avg_cosine_similarity = np.mean(top_5_similarities) if top_5_similarities else 0.0
        except Exception as e:
            logger.error(f"PlanID={self.plan_id} - Failed to calculate cosine similarity for metrics: {e}")
            avg_cosine_similarity = 0.0

        # 2. Calculate total tokens for summarization
        # A simple approximation: 1 token ~= 4 characters
        total_tokens = len(combined_context) // 4

        # 3. Save to log file
        log_entry = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "plan_id": self.plan_id,
            "avg_top5_cosine_similarity": f"{avg_cosine_similarity:.4f}",
            "tokens_for_summarization": total_tokens,
            "num_queries": len(queries),
            "num_docs_retrieved": len(docs)
        }

        try:
            with open(settings.BASE_DIR / 'logs/retrieval_quality_audit.log', 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry) + '\n')
        except Exception as e:
            logger.error(f"PlanID={self.plan_id} - Failed to write to retrieval_quality_audit.log: {e}")

    @log_execution_details
    def _perform_advanced_retrieval(self) -> Tuple[Dict[str, Any], Optional[str]]:
            step_name = "retrieval"
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Step 1: Performing advanced retrieval...")
            retrieval_service = RetrievalService(retriever=self.retriever, llm=self.fast_llm)
            errors = []

            try:
                queries = retrieval_service.generate_query_variations(self.raw_site_context)
            except Exception as e:
                msg = f"Query generation failed; using fallback queries. Error: {e}"
                logger.exception(f"PlanID={self.plan_id}, Step={step_name} - {msg}")
                errors.append(msg)
                queries = [
                    f"Freshwater regulations for {self.catchment}",
                    "Nutrient leaching risk assessment New Zealand"
                ]

            try:
                docs = retrieval_service.multi_query_retrieve(queries)
                if not docs:
                    logger.warning(f"PlanID={self.plan_id}, Step={step_name} - No documents retrieved.")
                    errors.append("No documents retrieved from vector store.")
                    # Return here as subsequent steps are not possible
                    return {"docs": [], "combined_context": ""}, " ".join(errors)

                trimmed_docs = self._trim_documents_to_token_budget(docs, max_tokens=self.CONTEXT_TOKEN_LIMIT)
                combined_context = self._docs_to_combined_context(trimmed_docs)
                
                # Log retrieval metrics
                self._log_retrieval_metrics(queries, trimmed_docs, combined_context)
                
                # Return docs and any non-fatal errors that occurred
                return {"docs": trimmed_docs, "combined_context": combined_context}, " ".join(errors) if errors else None
            except Exception as e:
                msg = f"Error during document retrieval: {e}"
                logger.exception(f"PlanID={self.plan_id}, Step={step_name} - {msg}")
                errors.append(msg)
                return {"docs": [], "combined_context": ""}, " ".join(errors)

        # ----------------------------------
        # Step 2: Summarize Retrieved Context
        # ----------------------------------
    @log_execution_details
    def _summarize_retrieved_context(self, docs: List[Any], combined_context: str) -> Tuple[str, Optional[str]]:
            step_name = "summarisation"
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Step 2: Summarizing retrieved context...")
            if not docs or not combined_context:
                return "Not applicable - no documents were retrieved.", "No documents to summarize"

            cache_key = self._cache_key_for_context(docs)
            cached = cache.get(cache_key)
            if cached:
                logger.debug(f"PlanID={self.plan_id}, Step={step_name} - Using cached regulatory summary.")
                return cached, None

            template = """
            You are an expert environmental consultant. Your task is to provide a concise, technical summary of the provided regulatory context documents.
            Focus on key regulations, policies, and guidelines relevant to freshwater management and farm planning in New Zealand, especially for {council} and {catchment}.
            
            Context:
            {context}

            Instructions:
            - Summarize the context in a formal, objective, and technical tone.
            - Avoid conversational language.
            - The summary should be approximately {word_target} words.
            - If specific regulatory details are not available in the context, state "Regulatory context not detailed in provided documents."
            - If no relevant information is found, state "No relevant regulatory context found in provided documents."
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | self.fast_llm | StrOutputParser()
            
            inputs = {"context": combined_context, "word_target": self.SUMMARY_WORD_TARGET, "council": self.council, "catchment": self.catchment}
            summary, error = self._safe_invoke(chain, inputs)

            if error:
                return "Error: Could not generate regulatory summary.", error
            
            if summary:
                cache.set(cache_key, summary, timeout=self.CACHE_TTL)
            
            return summary, None

        # ----------------------------------
        # Step 3: Summarize Catchment Context
        # ----------------------------------
    @log_execution_details
    def summarize_catchment_context(self, combined_context: str) -> Tuple[str, Optional[str]]:
            step_name = "catchment_summary"
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Step 3: Summarizing catchment context...")
            if not combined_context:
                return "Not applicable - no documents were retrieved.", "No context to summarize"

            template = """
            You are an expert environmental consultant. Your task is to provide a concise, technical summary of the environmental characteristics and context of the {catchment} area.
            Focus on key biophysical features, water quality, land use, and any specific environmental challenges or sensitivities mentioned in the provided documents.

            Context:
            {context}

            Instructions:
            - Summarize the catchment context in a formal, objective, and technical tone.
            - Avoid conversational language.
            - If specific details about the catchment are not available in the context, state "Catchment context not detailed in provided documents."
            - If no relevant information is found, state "No relevant catchment context found in provided documents."
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | self.fast_llm | StrOutputParser()

            inputs = {"context": combined_context, "catchment": self.catchment}
            summary, error = self._safe_invoke(chain, inputs)

            if error:
                return "Error: Could not generate catchment summary.", error
            
            return summary, None

        # ----------------------------------
        # Step 4: Identify Risks from Data
        # ----------------------------------
    @log_execution_details
    def identify_risks_from_data(self) -> Tuple[str, Optional[str]]:
            step_name = "risk_identification"
            logger.info(f"PlanID={self.plan_id}, Step={step_name} - Step 4: Identifying risks from site data...")
            template = """
            You are an expert environmental consultant. Your task is to identify potential environmental risks based on the provided site-specific data.
            Focus on risks related to freshwater quality, soil health, biodiversity, and regulatory compliance.

            Site Data (JSON):
            {site_data}

            Instructions:
            - Identify and list potential risks in a formal, objective, and technical tone.
            - Avoid conversational language.
            - If no specific risks can be identified from the provided data, state "No specific environmental risks identified from the provided site data."
            - If data is missing for a particular aspect, state "Risk identification limited due to missing data for [aspect]."
            """
            prompt = PromptTemplate.from_template(template)
            chain = prompt | self.fast_llm | StrOutputParser()

            inputs = {"site_data": json.dumps(self.site_context, indent=2)}
            risks, error = self._safe_invoke(chain, inputs)

            if error:
                return "Error: Could not identify risks from site data.", error
            
            return risks, None

        # ----------------------------------
        # Step 5: Generate Structured Analysis
        # ----------------------------------
    @log_execution_details
    def _generate_technical_summary(self, combined_context: str) -> Tuple[str, Optional[str]]:
        template = """
        You are an expert environmental consultant. Based on the provided context and site data, write a high-level, markdown-formatted technical summary.
        This should be a 3-4 paragraph executive summary of the property's environmental context and key risks.
        Context: {context}
        Site Data: {site_data}
        """
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.powerful_llm | StrOutputParser()
        inputs = {"context": combined_context, "site_data": json.dumps(self.site_context, indent=2)}
        return self._safe_invoke(chain, inputs)

    @log_execution_details
    def _generate_layer_explanations(self, combined_context: str) -> Tuple[Optional[Dict[str, str]], Optional[str]]:
        from pydantic import BaseModel, Field

        class LayerExplanations(BaseModel):
            land_use: str = Field(description='Detailed, markdown-formatted paragraph explaining what the land use data source reveals about the property. If data is missing or insufficient, return "Not available for this location."')
            erosion: str = Field(description='Detailed, markdown-formatted paragraph explaining what the erosion data source reveals about the property. If data is missing or insufficient, return "Not available for this location."')
            protected_areas: str = Field(description='Detailed, markdown-formatted paragraph explaining what the protected areas data source reveals about the property. If data is missing or insufficient, return "Not available for this location."')
            groundwater_zones: str = Field(description='Detailed, markdown-formatted paragraph explaining what the groundwater zones data source reveals about the property. If data is missing or insufficient, return "Not available for this location."')

        template = """
        You are an expert environmental consultant. Based on the provided context and site data, provide a detailed, markdown-formatted paragraph for each requested field explaining what the corresponding data source reveals about the property.

        Context:
        {context}

        Site Data:
        {site_data}
        """
        prompt = PromptTemplate.from_template(template)
        
        # Use with_structured_output for robust JSON generation
        structured_llm = self.powerful_llm.with_structured_output(LayerExplanations)
        chain = prompt | structured_llm
        
        inputs = {"context": combined_context, "site_data": json.dumps(self.site_context, indent=2)}
        
        parsed_output, error = self._safe_invoke(chain, inputs)
        
        if error:
            return None, error

        if isinstance(parsed_output, LayerExplanations):
            return parsed_output.dict(), None
        else:
            logger.error(f"PlanID={self.plan_id} - Layer explanations did not return a LayerExplanations object. Got: {type(parsed_output)}")
            return None, "Layer explanations generation returned an invalid format."

    @log_execution_details
    def _generate_biophysical_table(self, combined_context: str) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:
        from pydantic import BaseModel, Field
        from typing import List, Literal

        class BiophysicalAttribute(BaseModel):
            Attribute: str = Field(description="The name of the biophysical attribute (e.g., 'Mean Annual Rainfall', 'Dominant Soil Order', 'Slope Class').")
            Value: str = Field(description="The value or description of the attribute.")
            Source: str = Field(description="The source of the information (e.g., 'Document', 'Inferred from site context').")
            Confidence: Literal["High", "Medium", "Low"] = Field(description="The confidence level in the provided value.")

        class BiophysicalTable(BaseModel):
            table: List[BiophysicalAttribute] = Field(description="The list of biophysical attributes for the table.")

        # Template for both initial attempt and synthesis
        template = """
        You are an expert environmental consultant. Your task is to generate a structured JSON table of key biophysical attributes for a site.
        Use the provided context and site data. If information is not explicitly available in the documents, you MUST infer a plausible value based on the general site context (e.g., its location in Southland, New Zealand).

        For each attribute, provide the value, the source of the information, and your confidence level in the value.
        - Source should be 'Document' if explicitly found, or 'Inferred' if you deduced it from the site context.
        - Confidence should be 'High' for direct data, 'Medium' for strong inferences, and 'Low' for weak inferences.
        - Focus on attributes related to Climate, Hydrology, Soil, and Landform.

        Context:
        {context}

        Site Data:
        {site_data}
        """
        
        prompt = PromptTemplate.from_template(template)
        structured_llm = self.powerful_llm.with_structured_output(BiophysicalTable)
        chain = prompt | structured_llm
        
        # The same prompt works for both extraction and synthesis, as it encourages inference.
        inputs = {"context": combined_context, "site_data": json.dumps(self.site_context, indent=2)}
        
        parsed_output, error = self._safe_invoke(chain, inputs)
        
        if error:
            return [], error

        table_data = []
        if isinstance(parsed_output, BiophysicalTable) and parsed_output.table:
            table_data = [item.dict() for item in parsed_output.table]
        
        if not table_data:
             logger.warning(f"PlanID={self.plan_id} - Biophysical table generation returned an empty list, even with inference encouraged.")
             return [], "Biophysical table generation failed to produce data."


        logger.info(f"PlanID={self.plan_id} - Generated biophysical table with {len(table_data)} rows.")
        return table_data, None

    def generate_structured_analysis(self, catchment_summary: str, identified_risks: str, regulatory_context: str) -> Tuple[Dict[str, Any], Dict[str, Optional[str]]]:
        """
        Orchestrates the generation of the final structured analysis by calling smaller, independent LLM tasks.
        This replaces the single, monolithic LLM call.
        """
        step_name = "structured_analysis"
        logger.info(f"PlanID={self.plan_id}, Step={step_name} - Step 5: Generating final structured analysis via decomposed tasks...")

        # The combined context is the most important input for these generation steps.
        # We can also include the other summaries for a more complete picture.
        full_generation_context = f"""
        ### Regulatory Context
        {regulatory_context}

        ### Catchment Summary
        {catchment_summary}

        ### Identified Site Risks
        {identified_risks}
        """

        # --- Run Decomposed Generation Tasks ---
        tech_summary, tech_summary_error = self._generate_technical_summary(full_generation_context)
        layer_explanations, layer_explanations_error = self._generate_layer_explanations(full_generation_context)
        biophysical_table, biophysical_table_error = self._generate_biophysical_table(full_generation_context)

        # --- Assemble the Report and Errors ---
        structured_report = {
            "technical_summary": tech_summary or "Unknown - generation failed.",
            # The catchment summary is now passed in directly, not re-generated here.
            "catchment_context_summary": catchment_summary or "Unknown - generation failed.",
            "layer_explanations": layer_explanations or {
                "land_use": "Unknown - generation failed.",
                "erosion": "Unknown - generation failed.",
                "protected_areas": "Unknown - generation failed.",
                "groundwater_zones": "Unknown - generation failed.",
            },
            "biophysical_table": biophysical_table or [],
        }

        errors = {
            "technical_summary": tech_summary_error,
            "layer_explanations": layer_explanations_error,
            "biophysical_table": biophysical_table_error,
        }
        # Filter out non-error 'None' values
        filtered_errors = {k: v for k, v in errors.items() if v}

        # The second return value is now a dictionary of errors from the sub-steps.
        return structured_report, filtered_errors

    # ------------------------->
    # Orchestration
    # ------------------------->
    def run_full_analysis(self) -> Dict[str, Any]:
        logger.info(f"PlanID={self.plan_id} - Starting full, isolated-step vulnerability analysis pipeline...")
        
        error_details = {}
        
        # Step 1
        retrieval_result, retrieval_error = self._perform_advanced_retrieval()
        error_details["retrieval"] = "ok" if not retrieval_error else retrieval_error
        
        # Step 2
        summarized_regulatory_context, summarisation_error = self._summarize_retrieved_context(
            retrieval_result["docs"], retrieval_result["combined_context"]
        )
        error_details["summarisation"] = "ok" if not summarisation_error else summarisation_error

        # Step 3
        catchment_summary, catchment_error = self.summarize_catchment_context(retrieval_result["combined_context"])
        error_details["catchment_summary"] = "ok" if not catchment_error else catchment_error

        # Step 4
        identified_risks, risk_error = self.identify_risks_from_data()
        error_details["risk_identification"] = "ok" if not risk_error else risk_error

        # Step 5
        structured_report, structured_errors_dict = self.generate_structured_analysis(
            catchment_summary, identified_risks, summarized_regulatory_context
        )
        if structured_errors_dict:
            # We now have more granular errors
            error_details["structured_analysis"] = structured_errors_dict
        else:
            error_details["structured_analysis"] = "ok"

        # --- Quality Validation Layer ---
        quality_details = {}
        combined_context_str = retrieval_result.get("combined_context", "")

        def run_validation(step_name, text, context):
            status, reason, score = self._validate_output(text, context, step_name)
            details = {"status": status, "reason": reason}
            if score is not None:
                details["similarity"] = f"{score:.2f}"
            quality_details[step_name] = details

        run_validation("summarisation", summarized_regulatory_context, combined_context_str)
        run_validation("catchment_summary", catchment_summary, combined_context_str)
        if structured_report and structured_report.get("technical_summary"):
            run_validation("technical_summary", structured_report["technical_summary"], combined_context_str)
        
        site_context_str = json.dumps(self.site_context, indent=2)
        run_validation("risk_identification", identified_risks, site_context_str)
        
        error_details["quality_check"] = quality_details

        # Determine overall quality
        overall_quality = "✅ Valid"
        if any(d["status"] == "Invalid" for d in quality_details.values()):
            overall_quality = "❌ Invalid"
        elif any(d["status"] == "Degraded" for d in quality_details.values()):
            overall_quality = "⚠️ Degraded"

        # Determine overall status
        has_errors = any(v != "ok" for k, v in error_details.items() if k != 'quality_check' and not isinstance(v, dict)) or any(isinstance(v, dict) and v for k, v in error_details.items())
        status = "partial" if has_errors else "ok"
        if overall_quality != "✅ Valid" and status == "ok":
            status = "degraded"

        # Assemble final report data
        data = {
            "technical_summary": structured_report.get("technical_summary", "Unknown") if structured_report else "Unknown",
            "catchment_context_summary": catchment_summary,
            "summarized_regulatory_context": summarized_regulatory_context,
            "identified_risks": identified_risks,
            "layer_explanations": structured_report.get("layer_explanations", {}) if structured_report else {},
            "biophysical_table": structured_report.get("biophysical_table", []) if structured_report else [],
        }

        # Final report structure
        final_report = {
            "quality_summary": overall_quality,
            "status": status,
            "data": data,
            "error_details": error_details
        }

        return final_report

    # --- Helper methods for trimming and caching (unchanged) ---
    def _docs_to_combined_context(self, docs: List[Any]) -> str:
        parts = []
        for d in docs:
            content = getattr(d, "page_content", None) or getattr(d, "content", "")
            source = (getattr(d, "metadata", {}) or {}).get("source") if hasattr(d, "metadata") else None
            if source:
                parts.append(f"{content}\n\n[[SOURCE: {source}]]")
            else:
                parts.append(content)
        return "\n\n---\n\n".join(parts)

    def _trim_documents_to_token_budget(self, docs: List[Any], max_tokens: int) -> List[Any]:
        kept = []
        total = 0
        for d in docs:
            text = getattr(d, "page_content", None) or getattr(d, "content", "")
            t = max(1, len(text) // 4)
            if total + t > max_tokens:
                logger.info(f"Token budget reached. Trimming remaining {len(docs) - len(kept)} documents.")
                break
            kept.append(d)
            total += t
        logger.info(f"Trimmed docs to {len(kept)} chunks within token budget ({total}/{max_tokens} tokens).")
        return kept

    def _cache_key_for_context(self, docs: List[Any]) -> str:
        combined = "".join([(getattr(d, "page_content", "") or "")[:2000] for d in docs])
        h = hashlib.sha256((combined + str(self.catchment)).encode("utf-8")).hexdigest()
        return f"vuln:regcontext:{h}"
